---
title: "Phyloseq PreProcessing"
author: "Alicia McElwee"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: yes
    toc_float: yes
    collapsed: no
    smooth_scroll: yes
    toc_depth: 3
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.align = "center",
                      fig.path = "../figures/02_ProProcessing") # Sends any figure output to this folder
```


# Load Libraries
```{r install-libraries}

library(devtools)

#install_github("joey711/phyloseq")
library(phyloseq)

library(tidyverse)

```

# Goals

Here, we will process the data into a phyloseq object.

Three input files:

- ASV table
- Taxonomy table
- Track Reads (metadata)

Then, we will remove the following:

1.) Mitochondria
2.) Chloroplasts
3.) Remove samples without "enough" reads

Finally, write a data file of phyloseq output.



# Load Data

## ASV Table
```{r load-ASV-table}
# First, will load ASV table
load("data/01_DADA2/ASV_counts.RData")

# Inspect asv_tab
head(asv_tab)[,1:5]

# Fix sample names
sample_names <- colnames(asv_tab) # makes vector of sample filenames 
samples_fixed <- sapply(strsplit(basename(sample_names), "_"), `[`,1)
head(samples_fixed)

# re-write ASV count file to fix names
colnames(asv_tab) <- samples_fixed
str(asv_tab)

```

## Taxonomy Table
``` {r load-tax-table}
tax_df <- read.table("data/01_DADA2/ASV_taxonomy.tsv", sep = "\t", skip = 1)
head(tax_df)

# Fix column names
colnames(tax_df) <- c("asv_names","Kingdom","Phylum","Class", "Order", "Family", "Genus", "Species", "ASV", "ASV_seq")
head(tax_df)

# Make tax table a matrxi, Phyloseq needs tax table to be a matrix, requires row and column names
tax_mat <- tax_df %>%
  tibble::column_to_rownames(., var = "asv_names") %>% # takes asv_names column and makes it row names 
  as.matrix()


```

## Track Reads Data
``` {r load-track-reads}
load("data/01_DADA2/track_read_counts.RData")


```


# Handoff to Phyloseq

Make data into phyloseq object
```{r phyloseq-handoff}


```
